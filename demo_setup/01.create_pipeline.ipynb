{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bb40d85a-78e1-4a6b-8668-a60acf6deeec",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#Helper notebook to deploy the DLT pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "51a6dd60-f0a9-4f66-8a4b-5a575d4e8487",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ../00.set_variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7eddf02b-e52b-4e2f-b554-318ae5bc461f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Databricks Notebook Script for DLT Pipeline Deployment with Relative Path\n",
    "\n",
    "# --- 1. Install Databricks SDK (if not already available in your cluster) ---\n",
    "# Ensure you are running a recent version of the SDK.\n",
    "%pip install databricks-sdk\n",
    "\n",
    "# --- 2. Import necessary libraries ---\n",
    "from databricks.sdk import WorkspaceClient\n",
    "from databricks.sdk.service import pipelines\n",
    "import json\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# --- IMPORTANT: Access dbutils ---\n",
    "# dbutils is automatically available in Databricks notebooks.\n",
    "# No explicit import needed for 'dbutils' itself.\n",
    "\n",
    "# --- 3. Configuration Parameters ---\n",
    "pipeline_name = \"demo_iron_ore_processing\"\n",
    "# This is the relative path from the *current* notebook's directory\n",
    "relative_dlt_notebook_path = \"../notebooks/01a. Data Engineering\" # Assumes '01a. Data Engineering' is the DLT notebook\n",
    "\n",
    "# Environment variables to set on the pipeline\n",
    "environment_variables = {\n",
    "    \"catalog_name\": catalog_name,\n",
    "    \"schema_name\": schema_name,\n",
    "    \"volume_name\": volume_name\n",
    "}\n",
    "\n",
    "# --- 4. Determine the full path of the target DLT notebook ---\n",
    "try:\n",
    "    # Get the path of the current notebook using dbutils\n",
    "    # dbutils.notebook.current().path is the recommended and cleaner way\n",
    "    current_notebook_path = dbutils.entry_point.getDbutils().notebook().getContext().notebookPath().get()\n",
    "    print(f\"Current notebook path: {current_notebook_path}\")\n",
    "\n",
    "    # Use pathlib to handle path manipulation robustly\n",
    "    current_notebook_dir = Path(current_notebook_path).parent\n",
    "    full_dlt_notebook_path = (current_notebook_dir / relative_dlt_notebook_path).resolve()\n",
    "\n",
    "    # Convert the Path object back to a string for the DLT API\n",
    "    target_notebook_path = str(full_dlt_notebook_path)\n",
    "\n",
    "    print(f\"Resolved DLT target notebook path: {target_notebook_path}\")\n",
    "\n",
    "    # Basic validation: ensure it looks like a Databricks path\n",
    "    if not target_notebook_path.startswith(\"/\"):\n",
    "        raise ValueError(\"Resolved DLT path does not start with '/' which is required for Databricks workspace paths.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error determining target notebook path: {e}\")\n",
    "    raise # Re-raise to stop execution if path resolution fails\n",
    "\n",
    "# --- 5. Initialize Databricks WorkspaceClient ---\n",
    "w = WorkspaceClient()\n",
    "\n",
    "# Check if the pipeline exists\n",
    "existing_pipelines = w.pipelines.list_pipelines()\n",
    "\n",
    "try:\n",
    "    created = w.pipelines.create(\n",
    "        catalog=catalog_name,\n",
    "        schema=schema_name,\n",
    "        continuous=False,\n",
    "        name=pipeline_name,\n",
    "        libraries=[pipelines.PipelineLibrary(notebook=pipelines.NotebookLibrary(path=target_notebook_path))],\n",
    "        serverless=True,\n",
    "        configuration=environment_variables\n",
    "    )\n",
    "    print(\"creating a new pipeline\")\n",
    "    pipeline_id = created.pipeline_id\n",
    "#crude fallback to update to save time on initial creation\n",
    "except Exception as e:\n",
    "    print(f\"pipeline exists... updating existing got this error trying to create : {e}\")\n",
    "    pipeline_id = next(pipeline.id for pipeline in existing_pipelines if pipeline.name == pipeline_name)\n",
    "    updated = w.pipelines.update(\n",
    "        pipeline_id=pipeline_id,\n",
    "        catalog=catalog_name,\n",
    "        schema=schema_name,\n",
    "        continuous=False,\n",
    "        name=pipeline_name,\n",
    "        libraries=[pipelines.PipelineLibrary(notebook=pipelines.NotebookLibrary(path=target_notebook_path))],\n",
    "        serverless=True,\n",
    "        configuration=environment_variables\n",
    "    )\n",
    "    pipeline_id = created.pipeline_id\n",
    "    \n",
    "#run the new or updated pipeline\n",
    "run = w.pipelines.start_update(pipeline_id=pipeline_id)\n",
    "display(run)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "dbe_c75093c8-0895-475e-8c1b-6acacfe3368b",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "01.create_pipeline",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

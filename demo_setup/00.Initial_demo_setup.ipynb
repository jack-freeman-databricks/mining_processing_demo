{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "28eca99e-b715-4cfc-9050-e58fb9d2e616",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#Initial loading of dataset into demo volume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "abcfc802-a85a-4285-b3b5-539f58bbecae",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ../00.set_variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "617f4d38-ff5d-4e8d-b740-4b4a7b936ef3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create a catalog\n",
    "spark.sql(f\"CREATE CATALOG IF NOT EXISTS {catalog_name}\")\n",
    "\n",
    "# Create a schema within the catalog\n",
    "spark.sql(f\"CREATE SCHEMA IF NOT EXISTS {catalog_name}.{schema_name}\")\n",
    "\n",
    "#create a managed volume\n",
    "spark.sql(f\"CREATE VOLUME IF NOT EXISTS {catalog_name}.{schema_name}.{volume_name}\")\n",
    "\n",
    "# Grant all permissions on the catalog to all account users (crude but will avoid downstream issues)\n",
    "spark.sql(f\"GRANT ALL PRIVILEGES ON CATALOG {catalog_name} TO `account users`\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "765e39c8-0b8a-4b2d-95e0-cf7a4b38e378",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def create_mock_plant_equipment():\n",
    "    data = [\n",
    "        (\"Spectrophotometer\", 1, True),\n",
    "        (\"pH Meter\", 2, True),\n",
    "        (\"Oven\", 3, True),\n",
    "        (\"Remote Arm\", 4, False),\n",
    "        (\"Thermometer\", 5, None),\n",
    "    ]\n",
    "    \n",
    "    columns = [\"name\", \"id\", \"is_active\"]\n",
    "    df_equipment = spark.createDataFrame(data, columns)\n",
    "    \n",
    "    return df_equipment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "9b55a0f0-2b79-45ba-8c6e-22d295983085",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import zipfile\n",
    "import io\n",
    "\n",
    "# Download the zip file\n",
    "url = \"https://www.kaggle.com/api/v1/datasets/download/edumagalhaes/quality-prediction-in-a-mining-process\"\n",
    "response = requests.get(url)\n",
    "zip_file = zipfile.ZipFile(io.BytesIO(response.content))\n",
    "\n",
    "# Extract the CSV file to a UC volume\n",
    "csv_filename = \"MiningProcess_Flotation_Plant_Database.csv\"\n",
    "zip_file.extract(csv_filename, f\"/Volumes/{catalog_name}/{schema_name}/{volume_name}/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "d9d05b13-b995-4cd4-9d18-2c7666b047c4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Define the file path\n",
    "file_path = f\"/Volumes/{catalog_name}/{schema_name}/{volume_name}/MiningProcess_Flotation_Plant_Database.csv\"\n",
    "\n",
    "# Read the CSV file into a DataFrame\n",
    "df = spark.read.format(\"csv\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .load(file_path)\n",
    "\n",
    "from pyspark.sql.functions import col, hour, when, concat_ws, lit, avg\n",
    "from pyspark.sql import functions as F\n",
    "import random\n",
    "import pandas as pd\n",
    "\n",
    "# Rename columns to remove invalid characters\n",
    "df = df.select([F.col(col).alias(col.replace(\" \", \"_\").replace(\"%\", \"Percent\")) for col in df.columns])\n",
    "\n",
    "for col in df.columns:\n",
    "    if col != \"date\":\n",
    "        df = df.withColumn(col, F.regexp_replace(col, r\",\", \".\").cast(\"double\"))\n",
    "\n",
    "flotation_columns = [\n",
    "    \"date\",\n",
    "    \"Starch_Flow\",\n",
    "    \"Amina_Flow\", \n",
    "    \"Ore_Pulp_Flow\",\n",
    "    \"Ore_Pulp_pH\", \n",
    "    \"Ore_Pulp_Density\", \n",
    "    \"Flotation_Column_01_Air_Flow\", \n",
    "    \"Flotation_Column_02_Air_Flow\",\n",
    "    \"Flotation_Column_03_Air_Flow\",\n",
    "    \"Flotation_Column_04_Air_Flow\",\n",
    "    \"Flotation_Column_05_Air_Flow\",\n",
    "    \"Flotation_Column_06_Air_Flow\",\n",
    "    \"Flotation_Column_07_Air_Flow\",\n",
    "    \"Flotation_Column_01_Level\",\n",
    "    \"Flotation_Column_02_Level\",\n",
    "    \"Flotation_Column_03_Level\",\n",
    "    \"Flotation_Column_04_Level\",\n",
    "    \"Flotation_Column_05_Level\",\n",
    "    \"Flotation_Column_06_Level\",\n",
    "    \"Flotation_Column_07_Level\",\n",
    "]\n",
    "lab_data_columns = [\n",
    "    \"date\",\n",
    "    \"Percent_Iron_Feed\",\n",
    "    \"Percent_Silica_Feed\",\n",
    "    \"Percent_Iron_Concentrate\",\n",
    "    \"Percent_Silica_Concentrate\",\n",
    "]\n",
    "df_flotation = df.select(*flotation_columns)\n",
    "df_flotation.write.format(\"parquet\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .save(f\"/Volumes/{catalog_name}/{schema_name}/{volume_name}/flotation_data/\")\n",
    "\n",
    "df_lab = df.select(*lab_data_columns)\n",
    "\n",
    "df_lab_hourly = df_lab.groupBy(\"date\").agg(\n",
    "    avg(\"Percent_Iron_Feed\").alias(\"Percent_Iron_Feed\"),\n",
    "    avg(\"Percent_Silica_Feed\").alias(\"Percent_Silica_Feed\"),\n",
    "    avg(\"Percent_Iron_Concentrate\").alias(\"Percent_Iron_Concentrate\"),\n",
    "    avg(\"Percent_Silica_Concentrate\").alias(\"Percent_Silica_Concentrate\")\n",
    ")\n",
    "\n",
    "# Mock shift operators for PII demo\n",
    "\n",
    "# 1. List of fake operator names\n",
    "operator_names = [\n",
    "    \"Alice Johnson\", \"Ben Carter\", \"Cindy Lee\", \"David Smith\", \"Emma Wright\",\n",
    "    \"Frank Miller\", \"Grace Kim\", \"Henry Jones\", \"Isla Clarke\", \"Jack White\"\n",
    "]\n",
    "\n",
    "# Broadcast the list and create a shift ID based on date and shift\n",
    "df_with_shift = df_lab_hourly.withColumn(\"shift_type\", when(\n",
    "    (hour(\"date\") >= 6) & (hour(\"date\") < 18), \"day\"\n",
    ").otherwise(\"night\"))\n",
    "\n",
    "# Create a shift identifier (e.g., \"2024-05-13_day\")\n",
    "df_with_shift = df_with_shift.withColumn(\"shift_id\",\n",
    "    concat_ws(\"_\", F.to_date(\"date\"), F.col(\"shift_type\"))\n",
    ")\n",
    "\n",
    "# Get distinct shifts\n",
    "distinct_shifts = df_with_shift.select(\"shift_id\").distinct().collect()\n",
    "\n",
    "# Assign a random operator to each shift\n",
    "shift_operator_map = {row[\"shift_id\"]: random.choice(operator_names) for row in distinct_shifts}\n",
    "\n",
    "# Convert to a DataFrame for joining\n",
    "shift_df = spark.createDataFrame(shift_operator_map.items(), [\"shift_id\", \"operator_name\"])\n",
    "\n",
    "# Join operator name back to the main DataFrame\n",
    "df_with_operator = df_with_shift.join(shift_df, on=\"shift_id\", how=\"left\").drop(\"shift_id\")\n",
    "\n",
    "df_with_operator.write.format(\"parquet\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .save(f\"/Volumes/{catalog_name}/{schema_name}/{volume_name}/lab_data_hourly/\")\n",
    "\n",
    "#mock equipment with a None isactive field that we can use to demo expecations\n",
    "df_equipment = create_mock_plant_equipment()\n",
    "df_equipment.write.format(\"parquet\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .save(f\"/Volumes/{catalog_name}/{schema_name}/{volume_name}/equipment/\")\n",
    "\n",
    "#display(df_with_operator)\n",
    "#display(df_flotation)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4eb3bb60-3482-42e1-a4fe-eb1a85e0ba2f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ./01.create_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "52431d3f-d3d4-47db-a893-b2c65f230842",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ./02.create_dashboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8552717a-1fc2-4745-a52c-722f02c7667e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ./model_deploy_jobs/create-deployment-job"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 990384398379198,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 2
   },
   "notebookName": "00.Initial_demo_setup",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "28eca99e-b715-4cfc-9050-e58fb9d2e616",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###Creating initial resources:\n",
    "- source data\n",
    "- pipelines\n",
    "- dashboards\n",
    "- genie space\n",
    "- databricks app\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "55ede6cc-689c-49b9-bd9e-3016e1e5f656",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install databricks-sdk>=0.57.0 \n",
    "dbutils.library.restartPython() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "abcfc802-a85a-4285-b3b5-539f58bbecae",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ../00.set_variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "617f4d38-ff5d-4e8d-b740-4b4a7b936ef3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create a catalog, schema and volume\n",
    "\n",
    "spark.sql(f\"CREATE CATALOG IF NOT EXISTS {catalog_name}\")\n",
    "spark.sql(f\"CREATE SCHEMA IF NOT EXISTS {catalog_name}.{schema_name}\")\n",
    "spark.sql(f\"CREATE VOLUME IF NOT EXISTS {catalog_name}.{schema_name}.{volume_name}\")\n",
    "\n",
    "# Grant all permissions on the catalog to all account users (crude but will avoid downstream issues)\n",
    "spark.sql(f\"GRANT ALL PRIVILEGES ON CATALOG {catalog_name} TO `account users`\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "d9d05b13-b995-4cd4-9d18-2c7666b047c4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Create files from kaggle dataset in volumes\n",
    "\n",
    "import requests\n",
    "import zipfile\n",
    "import io\n",
    "\n",
    "def create_mock_plant_equipment():\n",
    "    data = [\n",
    "        (\"Spectrophotometer\", 1, True),\n",
    "        (\"pH Meter\", 2, True),\n",
    "        (\"Oven\", 3, True),\n",
    "        (\"Remote Arm\", 4, False),\n",
    "        (\"Thermometer\", 5, None),\n",
    "    ]\n",
    "    \n",
    "    columns = [\"name\", \"id\", \"is_active\"]\n",
    "    df_equipment = spark.createDataFrame(data, columns)\n",
    "    \n",
    "    return df_equipment\n",
    "\n",
    "# Download the zip file\n",
    "url = \"https://www.kaggle.com/api/v1/datasets/download/edumagalhaes/quality-prediction-in-a-mining-process\"\n",
    "response = requests.get(url)\n",
    "zip_file = zipfile.ZipFile(io.BytesIO(response.content))\n",
    "\n",
    "# Extract the CSV file to a UC volume\n",
    "csv_filename = \"MiningProcess_Flotation_Plant_Database.csv\"\n",
    "zip_file.extract(csv_filename, f\"/Volumes/{catalog_name}/{schema_name}/{volume_name}/\")\n",
    "\n",
    "# Define the file path\n",
    "file_path = f\"/Volumes/{catalog_name}/{schema_name}/{volume_name}/MiningProcess_Flotation_Plant_Database.csv\"\n",
    "\n",
    "# Read the CSV file into a DataFrame\n",
    "df = spark.read.format(\"csv\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .load(file_path)\n",
    "\n",
    "from pyspark.sql.functions import col, hour, when, concat_ws, lit, avg\n",
    "from pyspark.sql import functions as F\n",
    "import random\n",
    "import pandas as pd\n",
    "\n",
    "# Rename columns to remove invalid characters\n",
    "df = df.select([F.col(col).alias(col.replace(\" \", \"_\").replace(\"%\", \"Percent\")) for col in df.columns])\n",
    "\n",
    "for col in df.columns:\n",
    "    if col != \"date\":\n",
    "        df = df.withColumn(col, F.regexp_replace(col, r\",\", \".\").cast(\"double\"))\n",
    "\n",
    "flotation_columns = [\n",
    "    \"date\",\n",
    "    \"Starch_Flow\",\n",
    "    \"Amina_Flow\", \n",
    "    \"Ore_Pulp_Flow\",\n",
    "    \"Ore_Pulp_pH\", \n",
    "    \"Ore_Pulp_Density\", \n",
    "    \"Flotation_Column_01_Air_Flow\", \n",
    "    \"Flotation_Column_02_Air_Flow\",\n",
    "    \"Flotation_Column_03_Air_Flow\",\n",
    "    \"Flotation_Column_04_Air_Flow\",\n",
    "    \"Flotation_Column_05_Air_Flow\",\n",
    "    \"Flotation_Column_06_Air_Flow\",\n",
    "    \"Flotation_Column_07_Air_Flow\",\n",
    "    \"Flotation_Column_01_Level\",\n",
    "    \"Flotation_Column_02_Level\",\n",
    "    \"Flotation_Column_03_Level\",\n",
    "    \"Flotation_Column_04_Level\",\n",
    "    \"Flotation_Column_05_Level\",\n",
    "    \"Flotation_Column_06_Level\",\n",
    "    \"Flotation_Column_07_Level\",\n",
    "]\n",
    "lab_data_columns = [\n",
    "    \"date\",\n",
    "    \"Percent_Iron_Feed\",\n",
    "    \"Percent_Silica_Feed\",\n",
    "    \"Percent_Iron_Concentrate\",\n",
    "    \"Percent_Silica_Concentrate\",\n",
    "]\n",
    "df_flotation = df.select(*flotation_columns)\n",
    "df_flotation.write.format(\"parquet\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .save(f\"/Volumes/{catalog_name}/{schema_name}/{volume_name}/flotation_data/\")\n",
    "\n",
    "df_lab = df.select(*lab_data_columns)\n",
    "\n",
    "df_lab_hourly = df_lab.groupBy(\"date\").agg(\n",
    "    avg(\"Percent_Iron_Feed\").alias(\"Percent_Iron_Feed\"),\n",
    "    avg(\"Percent_Silica_Feed\").alias(\"Percent_Silica_Feed\"),\n",
    "    avg(\"Percent_Iron_Concentrate\").alias(\"Percent_Iron_Concentrate\"),\n",
    "    avg(\"Percent_Silica_Concentrate\").alias(\"Percent_Silica_Concentrate\")\n",
    ")\n",
    "\n",
    "# Mock shift operators for PII demo\n",
    "\n",
    "# 1. List of fake operator names\n",
    "operator_names = [\n",
    "    \"Alice Johnson\", \"Ben Carter\", \"Cindy Lee\", \"David Smith\", \"Emma Wright\",\n",
    "    \"Frank Miller\", \"Grace Kim\", \"Henry Jones\", \"Isla Clarke\", \"Jack White\"\n",
    "]\n",
    "\n",
    "# Broadcast the list and create a shift ID based on date and shift\n",
    "df_with_shift = df_lab_hourly.withColumn(\"shift_type\", when(\n",
    "    (hour(\"date\") >= 6) & (hour(\"date\") < 18), \"day\"\n",
    ").otherwise(\"night\"))\n",
    "\n",
    "# Create a shift identifier (e.g., \"2024-05-13_day\")\n",
    "df_with_shift = df_with_shift.withColumn(\"shift_id\",\n",
    "    concat_ws(\"_\", F.to_date(\"date\"), F.col(\"shift_type\"))\n",
    ")\n",
    "\n",
    "# Get distinct shifts\n",
    "distinct_shifts = df_with_shift.select(\"shift_id\").distinct().collect()\n",
    "\n",
    "# Assign a random operator to each shift\n",
    "shift_operator_map = {row[\"shift_id\"]: random.choice(operator_names) for row in distinct_shifts}\n",
    "\n",
    "# Convert to a DataFrame for joining\n",
    "shift_df = spark.createDataFrame(shift_operator_map.items(), [\"shift_id\", \"operator_name\"])\n",
    "\n",
    "# Join operator name back to the main DataFrame\n",
    "df_with_operator = df_with_shift.join(shift_df, on=\"shift_id\", how=\"left\").drop(\"shift_id\")\n",
    "\n",
    "df_with_operator.write.format(\"parquet\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .save(f\"/Volumes/{catalog_name}/{schema_name}/{volume_name}/lab_data_hourly/\")\n",
    "\n",
    "#mock equipment with a None isactive field that we can use to demo expecations\n",
    "df_equipment = create_mock_plant_equipment()\n",
    "df_equipment.write.format(\"parquet\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .save(f\"/Volumes/{catalog_name}/{schema_name}/{volume_name}/equipment/\")\n",
    "\n",
    "#display(df_with_operator)\n",
    "#display(df_flotation)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "4eb3bb60-3482-42e1-a4fe-eb1a85e0ba2f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ./01.create_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "52431d3f-d3d4-47db-a893-b2c65f230842",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ./02.create_dashboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "8552717a-1fc2-4745-a52c-722f02c7667e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ./model_deploy_jobs/create-deployment-job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "a0f8a249-26cd-47e8-b061-7b278ff9da62",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run \"../notebooks/01b. Unity Catalog, Governance and Auditability\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "49587269-f855-4e4a-bc2d-f774284f1d7d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Manually create feature store table ahead of time\n",
    "gold_df = spark.sql(f\"SELECT * FROM {catalog_name}.{schema_name}.gold_iron_ore_prediction_dataset\")\n",
    "\n",
    "# Filter out rows between the specified dates\n",
    "start_date = \"2017-05-12\"\n",
    "end_date = \"2017-06-14\"\n",
    "df_filtered = gold_df.filter(~(col(\"date\").between(start_date, end_date)))\n",
    "start_date = \"2017-07-23\"\n",
    "end_date = \"2017-08-03\"\n",
    "df_filtered = df_filtered.filter(~(col(\"date\").between(start_date, end_date)))\n",
    "start_date = \"2017-08-07\"\n",
    "end_date = \"2017-08-14\"\n",
    "df_filtered = df_filtered.filter(~(col(\"date\").between(start_date, end_date)))\n",
    "df_filtered = df_filtered.filter(col(\"Percent_Iron_Feed\") > 0)\n",
    "\n",
    "feature_table_name = \"fs_gold_iop_features\"\n",
    "\n",
    "from pyspark.sql.types import TimestampType\n",
    "\n",
    "df_filtered.write.format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"overwriteSchema\", \"true\") \\\n",
    "    .saveAsTable(f\"{catalog_name}.{schema_name}.{feature_table_name}\")\n",
    "\n",
    "spark.sql(f\"ALTER TABLE {catalog_name}.{schema_name}.{feature_table_name} ALTER COLUMN date SET NOT NULL\")\n",
    "spark.sql(f\"ALTER TABLE {catalog_name}.{schema_name}.{feature_table_name} ADD CONSTRAINT pk_date PRIMARY KEY (date)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "a35bd04b-76cf-4bf6-a66b-a54ed3f6b017",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run \"../notebooks/03. Model Training and Experimentation\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "e26a21bc-c161-4bbf-b4a4-25db0183be44",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ./03.create_genie_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "55640bbe-f9f8-40bf-b705-083675746871",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ./04.create_app"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "bd9aa9c0-c56d-4de4-ab36-ff36ab995c46",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ./05.create_serving_endpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "053322bf-d4a9-4a54-a8d7-5d762f027b41",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run \"../notebooks/05. Optimisation\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "4d17ccf2-8e84-41df-8503-9b5d395f8a9b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ./06.enable_data_classification_and_anomaly_detection"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": -1,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 2
   },
   "notebookName": "00.Initial_demo_setup",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "batchId": -3397375242293670,
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "28eca99e-b715-4cfc-9050-e58fb9d2e616",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###Creating initial resources:\n",
    "- source data\n",
    "- pipelines\n",
    "- dashboards\n",
    "- genie space\n",
    "- databricks app\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "abcfc802-a85a-4285-b3b5-539f58bbecae",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ../00.set_variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "617f4d38-ff5d-4e8d-b740-4b4a7b936ef3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create a catalog, schema and volume\n",
    "\n",
    "spark.sql(f\"CREATE CATALOG IF NOT EXISTS {catalog_name}\")\n",
    "spark.sql(f\"CREATE SCHEMA IF NOT EXISTS {catalog_name}.{schema_name}\")\n",
    "spark.sql(f\"CREATE VOLUME IF NOT EXISTS {catalog_name}.{schema_name}.{volume_name}\")\n",
    "\n",
    "# Grant all permissions on the catalog to all account users (crude but will avoid downstream issues)\n",
    "spark.sql(f\"GRANT ALL PRIVILEGES ON CATALOG {catalog_name} TO `account users`\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "d9d05b13-b995-4cd4-9d18-2c7666b047c4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Create files from kaggle dataset in volumes\n",
    "\n",
    "import requests\n",
    "import zipfile\n",
    "import io\n",
    "\n",
    "def create_mock_plant_equipment():\n",
    "    data = [\n",
    "        (\"Spectrophotometer\", 1, True),\n",
    "        (\"pH Meter\", 2, True),\n",
    "        (\"Oven\", 3, True),\n",
    "        (\"Remote Arm\", 4, False),\n",
    "        (\"Thermometer\", 5, None),\n",
    "    ]\n",
    "    \n",
    "    columns = [\"name\", \"id\", \"is_active\"]\n",
    "    df_equipment = spark.createDataFrame(data, columns)\n",
    "    \n",
    "    return df_equipment\n",
    "\n",
    "# Download the zip file\n",
    "url = \"https://www.kaggle.com/api/v1/datasets/download/edumagalhaes/quality-prediction-in-a-mining-process\"\n",
    "response = requests.get(url)\n",
    "zip_file = zipfile.ZipFile(io.BytesIO(response.content))\n",
    "\n",
    "# Extract the CSV file to a UC volume\n",
    "csv_filename = \"MiningProcess_Flotation_Plant_Database.csv\"\n",
    "zip_file.extract(csv_filename, f\"/Volumes/{catalog_name}/{schema_name}/{volume_name}/\")\n",
    "\n",
    "# Define the file path\n",
    "file_path = f\"/Volumes/{catalog_name}/{schema_name}/{volume_name}/MiningProcess_Flotation_Plant_Database.csv\"\n",
    "\n",
    "# Read the CSV file into a DataFrame\n",
    "df = spark.read.format(\"csv\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .load(file_path)\n",
    "\n",
    "from pyspark.sql.functions import col, hour, when, concat_ws, lit, avg\n",
    "from pyspark.sql import functions as F\n",
    "import random\n",
    "import pandas as pd\n",
    "\n",
    "# Rename columns to remove invalid characters\n",
    "df = df.select([F.col(col).alias(col.replace(\" \", \"_\").replace(\"%\", \"Percent\")) for col in df.columns])\n",
    "\n",
    "for col in df.columns:\n",
    "    if col != \"date\":\n",
    "        df = df.withColumn(col, F.regexp_replace(col, r\",\", \".\").cast(\"double\"))\n",
    "\n",
    "flotation_columns = [\n",
    "    \"date\",\n",
    "    \"Starch_Flow\",\n",
    "    \"Amina_Flow\", \n",
    "    \"Ore_Pulp_Flow\",\n",
    "    \"Ore_Pulp_pH\", \n",
    "    \"Ore_Pulp_Density\", \n",
    "    \"Flotation_Column_01_Air_Flow\", \n",
    "    \"Flotation_Column_02_Air_Flow\",\n",
    "    \"Flotation_Column_03_Air_Flow\",\n",
    "    \"Flotation_Column_04_Air_Flow\",\n",
    "    \"Flotation_Column_05_Air_Flow\",\n",
    "    \"Flotation_Column_06_Air_Flow\",\n",
    "    \"Flotation_Column_07_Air_Flow\",\n",
    "    \"Flotation_Column_01_Level\",\n",
    "    \"Flotation_Column_02_Level\",\n",
    "    \"Flotation_Column_03_Level\",\n",
    "    \"Flotation_Column_04_Level\",\n",
    "    \"Flotation_Column_05_Level\",\n",
    "    \"Flotation_Column_06_Level\",\n",
    "    \"Flotation_Column_07_Level\",\n",
    "]\n",
    "lab_data_columns = [\n",
    "    \"date\",\n",
    "    \"Percent_Iron_Feed\",\n",
    "    \"Percent_Silica_Feed\",\n",
    "    \"Percent_Iron_Concentrate\",\n",
    "    \"Percent_Silica_Concentrate\",\n",
    "]\n",
    "df_flotation = df.select(*flotation_columns)\n",
    "df_flotation.write.format(\"parquet\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .save(f\"/Volumes/{catalog_name}/{schema_name}/{volume_name}/flotation_data/\")\n",
    "\n",
    "df_lab = df.select(*lab_data_columns)\n",
    "\n",
    "df_lab_hourly = df_lab.groupBy(\"date\").agg(\n",
    "    avg(\"Percent_Iron_Feed\").alias(\"Percent_Iron_Feed\"),\n",
    "    avg(\"Percent_Silica_Feed\").alias(\"Percent_Silica_Feed\"),\n",
    "    avg(\"Percent_Iron_Concentrate\").alias(\"Percent_Iron_Concentrate\"),\n",
    "    avg(\"Percent_Silica_Concentrate\").alias(\"Percent_Silica_Concentrate\")\n",
    ")\n",
    "\n",
    "# Mock shift operators for PII demo\n",
    "\n",
    "# 1. List of fake operator names\n",
    "operator_names = [\n",
    "    \"Alice Johnson\", \"Ben Carter\", \"Cindy Lee\", \"David Smith\", \"Emma Wright\",\n",
    "    \"Frank Miller\", \"Grace Kim\", \"Henry Jones\", \"Isla Clarke\", \"Jack White\"\n",
    "]\n",
    "\n",
    "# Broadcast the list and create a shift ID based on date and shift\n",
    "df_with_shift = df_lab_hourly.withColumn(\"shift_type\", when(\n",
    "    (hour(\"date\") >= 6) & (hour(\"date\") < 18), \"day\"\n",
    ").otherwise(\"night\"))\n",
    "\n",
    "# Create a shift identifier (e.g., \"2024-05-13_day\")\n",
    "df_with_shift = df_with_shift.withColumn(\"shift_id\",\n",
    "    concat_ws(\"_\", F.to_date(\"date\"), F.col(\"shift_type\"))\n",
    ")\n",
    "\n",
    "# Get distinct shifts\n",
    "distinct_shifts = df_with_shift.select(\"shift_id\").distinct().collect()\n",
    "\n",
    "# Assign a random operator to each shift\n",
    "shift_operator_map = {row[\"shift_id\"]: random.choice(operator_names) for row in distinct_shifts}\n",
    "\n",
    "# Convert to a DataFrame for joining\n",
    "shift_df = spark.createDataFrame(shift_operator_map.items(), [\"shift_id\", \"operator_name\"])\n",
    "\n",
    "# Join operator name back to the main DataFrame\n",
    "df_with_operator = df_with_shift.join(shift_df, on=\"shift_id\", how=\"left\").drop(\"shift_id\")\n",
    "\n",
    "df_with_operator.write.format(\"parquet\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .save(f\"/Volumes/{catalog_name}/{schema_name}/{volume_name}/lab_data_hourly/\")\n",
    "\n",
    "#mock equipment with a None isactive field that we can use to demo expecations\n",
    "df_equipment = create_mock_plant_equipment()\n",
    "df_equipment.write.format(\"parquet\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .save(f\"/Volumes/{catalog_name}/{schema_name}/{volume_name}/equipment/\")\n",
    "\n",
    "#display(df_with_operator)\n",
    "#display(df_flotation)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a0e8fe76-627a-4515-8240-5b9eb254fe97",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "current_notebook_path = dbutils.entry_point.getDbutils().notebook().getContext().notebookPath().get()\n",
    "current_notebook_dir = Path(current_notebook_path).parent.parent\n",
    "current_notebook_dir = str(current_notebook_dir)\n",
    "print(f\"current notebook location: {current_notebook_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1f5f7962-df0b-4d26-b344-424d1eaab454",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from databricks.sdk import WorkspaceClient\n",
    "from databricks.sdk.service.jobs import JobSettings\n",
    "\n",
    "job_name = f\"deploy-iops-demo-{schema_name}\"\n",
    "notebook_path_1 = \"/Workspace/Users/your.name@databricks.com/my-notebook\"\n",
    "notebook_path_2 = \"/Workspace/Users/your.name@databricks.com/my-second-notebook\"\n",
    "task_key_1 = \"my-task\"\n",
    "task_key_2 = \"my-second-task\"\n",
    "\n",
    "job_settings = JobSettings.from_dict({\n",
    "    \"name\": job_name,\n",
    "    \"tasks\": [\n",
    "        {\n",
    "            \"task_key\": \"create-pipeline-and-ingest\",\n",
    "            \"notebook_task\": {\n",
    "                \"notebook_path\": f\"{current_notebook_dir}/demo_setup/01.create_pipeline\",\n",
    "                \"source\": \"WORKSPACE\"\n",
    "            }\n",
    "        },\n",
    "        {\n",
    "            \"task_key\": \"create-dashboard\",\n",
    "            \"notebook_task\": {\n",
    "                \"notebook_path\": f\"{current_notebook_dir}/demo_setup/02.create_dashboard\",\n",
    "                \"source\": \"WORKSPACE\"\n",
    "            },\n",
    "            \"depends_on\": [{\"task_key\": \"create-pipeline-and-ingest\"}]\n",
    "        },\n",
    "        {\n",
    "            \"task_key\": \"create-deployment-job\",\n",
    "            \"notebook_task\": {\n",
    "                \"notebook_path\": f\"{current_notebook_dir}/demo_setup/model_deploy_jobs/create-deployment-job \",\n",
    "                \"source\": \"WORKSPACE\"\n",
    "            },\n",
    "            \"depends_on\": [{\"task_key\": \"create-dashboard\"}]\n",
    "        },\n",
    "        {\n",
    "            \"task_key\": \"run-governance-notebook\",\n",
    "            \"notebook_task\": {\n",
    "                \"notebook_path\": f\"{current_notebook_dir}/notebooks/01b. Unity Catalog, Governance and Auditability\",\n",
    "                \"source\": \"WORKSPACE\"\n",
    "            },\n",
    "            \"depends_on\": [{\"task_key\": \"create-deployment-job\"}]\n",
    "        },\n",
    "        {\n",
    "            \"task_key\": \"run-eda-featurestore-notebook\",\n",
    "            \"notebook_task\": {\n",
    "                \"notebook_path\": f\"{current_notebook_dir}/notebooks/02. EDA and Feature Store\",\n",
    "                \"source\": \"WORKSPACE\"\n",
    "            },\n",
    "            \"depends_on\": [{\"task_key\": \"run-governance-notebook\"}]\n",
    "        },\n",
    "        {\n",
    "            \"task_key\": \"run-model-creation-notebook\",\n",
    "            \"notebook_task\": {\n",
    "                \"notebook_path\": f\"{current_notebook_dir}/notebooks/03. Model Training and Experimentation\",\n",
    "                \"source\": \"WORKSPACE\"\n",
    "            },\n",
    "            \"depends_on\": [{\"task_key\": \"run-eda-featurestore-notebook\"}]\n",
    "        },\n",
    "        {\n",
    "            \"task_key\": \"create-genie-space\",\n",
    "            \"notebook_task\": {\n",
    "                \"notebook_path\": f\"{current_notebook_dir}/demo_setup/03.create_genie_space\",\n",
    "                \"source\": \"WORKSPACE\"\n",
    "            },\n",
    "            \"depends_on\": [{\"task_key\": \"run-model-creation-notebook\"}]\n",
    "        },\n",
    "        {\n",
    "            \"task_key\": \"create-app\",\n",
    "            \"notebook_task\": {\n",
    "                \"notebook_path\": f\"{current_notebook_dir}/demo_setup/04.create_app\",\n",
    "                \"source\": \"WORKSPACE\"\n",
    "            },\n",
    "            \"depends_on\": [{\"task_key\": \"run-model-creation-notebook\"}]\n",
    "        },\n",
    "        {\n",
    "            \"task_key\": \"create-serving-endpoints\",\n",
    "            \"notebook_task\": {\n",
    "                \"notebook_path\": f\"{current_notebook_dir}/demo_setup/05.create_serving_endpoints \",\n",
    "                \"source\": \"WORKSPACE\"\n",
    "            },\n",
    "            \"depends_on\": [{\"task_key\": \"run-model-creation-notebook\"}]\n",
    "        },\n",
    "        {\n",
    "            \"task_key\": \"run-optimization-notebook\",\n",
    "            \"notebook_task\": {\n",
    "                \"notebook_path\": f\"{current_notebook_dir}/notebooks/05. Optimisation\",\n",
    "                \"source\": \"WORKSPACE\"\n",
    "            },\n",
    "            \"depends_on\": [{\"task_key\": \"run-model-creation-notebook\"}]\n",
    "        },\n",
    "        {\n",
    "            \"task_key\": \"enable-anomaly-detection-classification\",\n",
    "            \"notebook_task\": {\n",
    "                \"notebook_path\": f\"{current_notebook_dir}/demo_setup/06.enable_data_classification_and_anomaly_detection\",\n",
    "                \"source\": \"WORKSPACE\"\n",
    "            },\n",
    "            \"depends_on\": [{\"task_key\": \"run-model-creation-notebook\"}]\n",
    "        }\n",
    "    ]\n",
    "})\n",
    "\n",
    "w = WorkspaceClient()\n",
    "job = w.jobs.create(**job_settings.as_shallow_dict())\n",
    "print(f\"View the job at {w.config.host}/#job/{job.job_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "2217ec8e-bd5a-4de6-82ba-611d0eb57367",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "run = w.jobs.run_now(job_id=job.job_id)\n",
    "run_id = run.run_id\n",
    "\n",
    "while True:\n",
    "    run_status = w.jobs.get_run(run_id=run_id)\n",
    "    life_cycle_state = run_status.state.life_cycle_state\n",
    "    result_state = run_status.state.result_state\n",
    "    print(f\"View the job run at {w.config.host}/#job/{job.job_id}/run/{run_id}\")\n",
    "    print(f\"Run ID is still going: {run_id}, Life Cycle State: {life_cycle_state}, Result State: {result_state}\")\n",
    "    if result_state is not None:\n",
    "        break\n",
    "    time.sleep(30)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 3397375242600357,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 2
   },
   "notebookName": "00.Initial_demo_setup",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

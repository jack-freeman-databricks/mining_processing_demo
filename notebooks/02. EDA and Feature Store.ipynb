{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "b207692b-f7c1-4591-982d-1e73c50d37ef",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ../demo_setup/00.Initial_library_install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a81210e5-cfc7-495e-a496-b6e48d70f641",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "from databricks.feature_store import FeatureStoreClient\n",
    "\n",
    "from pprint import pprint\n",
    "from databricks.sdk import WorkspaceClient\n",
    "from databricks.sdk.service.catalog import *\n",
    "\n",
    "fs = FeatureStoreClient()\n",
    "w = WorkspaceClient()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7d604ca6-f4b6-4b81-98de-710df94b5661",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# 2. EDA and Feature Store\n",
    "\n",
    "In this section, we‚Äôll demonstrate how Databricks Notebooks accelerate exploratory data analysis (EDA) with native visualisations, and how to seamlessly store and manage feature tables using the Databricks Feature Store ‚Äî including the ability to serve features in real time via the online feature store for low-latency model inference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a09b87fa-08ae-48c1-89a6-63c34a8635ff",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 2.1 EDA\n",
    "\n",
    "\n",
    "- We now load the dataset produced by the ingestion and data engineering pipelines. This will serve as the foundation for exploratory analysis, data cleaning and feature identification.\n",
    "- We will use the `display()` fuction in Databricks, which is a powerful tool for exploratory data analysis (EDA), allowing users to interactively explore DataFrames through sortable tables, built-in visualisations (e.g., bar charts, histograms, scatter plots), and summary statistics ‚Äî all without writing additional code. It makes it easy to quickly spot trends, outliers, and data quality issues during early stages of analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c037b694-62ca-47e9-9cf4-df177249c8bb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Databricks visualization. Run in Databricks to view."
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1.subcommand+json": {
       "baseErrorDetails": null,
       "bindings": {},
       "collapsed": false,
       "command": "%python\n__backend_agg_display_orig = display\n__backend_agg_dfs = []\ndef __backend_agg_display_new(df):\n    __backend_agg_df_modules = [\"pandas.core.frame\", \"databricks.koalas.frame\", \"pyspark.sql.dataframe\", \"pyspark.pandas.frame\", \"pyspark.sql.connect.dataframe\"]\n    if (type(df).__module__ in __backend_agg_df_modules and type(df).__name__ == 'DataFrame') or isinstance(df, list):\n        __backend_agg_dfs.append(df)\n\ndisplay = __backend_agg_display_new\n\ndef __backend_agg_user_code_fn():\n    import base64\n    exec(base64.standard_b64decode(\"Z29sZF9kZiA9IHNwYXJrLnNxbChmIlNFTEVDVCAqIEZST00ge0RFRkFVTFRfQ0FUQUxPR30ue0RFRkFVTFRfU0NIRU1BfS5nb2xkX2lyb25fb3JlX3ByZWRpY3Rpb25fZGF0YXNldCIpCmRpc3BsYXkoZ29sZF9kZik=\").decode())\n\ntry:\n    # run user code\n    __backend_agg_user_code_fn()\n\n    #reset display function\n    display = __backend_agg_display_orig\n\n    if len(__backend_agg_dfs) > 0:\n        # create a temp view\n        if type(__backend_agg_dfs[0]).__module__ == \"databricks.koalas.frame\":\n            # koalas dataframe\n            __backend_agg_dfs[0].to_spark().createOrReplaceTempView(\"DatabricksView5200b1f\")\n        elif type(__backend_agg_dfs[0]).__module__ == \"pandas.core.frame\" or isinstance(__backend_agg_dfs[0], list):\n            # pandas dataframe\n            spark.createDataFrame(__backend_agg_dfs[0]).createOrReplaceTempView(\"DatabricksView5200b1f\")\n        else:\n            __backend_agg_dfs[0].createOrReplaceTempView(\"DatabricksView5200b1f\")\n        #run backend agg\n        display(spark.sql(\"\"\"WITH q AS (select * from DatabricksView5200b1f) SELECT DATE_TRUNC('HOUR',`date`) `column_ef3e62491350`,AVG(`Percent_Iron_Feed`) `column_ef3e62491344`,AVG(`Percent_Silica_Feed`) `column_ef3e62491347` FROM q GROUP BY `column_ef3e62491350`\"\"\"))\n    else:\n        displayHTML(\"dataframe no longer exists. If you're using dataframe.display(), use display(dataframe) instead.\")\n\n\nfinally:\n    spark.sql(\"drop view if exists DatabricksView5200b1f\")\n    display = __backend_agg_display_orig\n    del __backend_agg_display_new\n    del __backend_agg_display_orig\n    del __backend_agg_dfs\n    del __backend_agg_user_code_fn\n\n",
       "commandTitle": "Feed",
       "commandType": "auto",
       "commandVersion": 0,
       "commentThread": [],
       "commentsVisible": false,
       "contentSha256Hex": null,
       "customPlotOptions": {
        "redashChart": [
         {
          "key": "type",
          "value": "CHART"
         },
         {
          "key": "options",
          "value": {
           "alignYAxesAtZero": true,
           "coefficient": 1,
           "columnConfigurationMap": {
            "x": {
             "column": "date",
             "id": "column_ef3e62491350",
             "transform": "HOUR_LEVEL"
            },
            "y": [
             {
              "column": "Percent_Iron_Feed",
              "id": "column_ef3e62491344",
              "transform": "AVG"
             },
             {
              "column": "Percent_Silica_Feed",
              "id": "column_ef3e62491347",
              "transform": "AVG"
             }
            ]
           },
           "dateTimeFormat": "DD/MM/YYYY HH:mm",
           "direction": {
            "type": "counterclockwise"
           },
           "error_y": {
            "type": "data",
            "visible": true
           },
           "globalSeriesType": "line",
           "isAggregationOn": true,
           "legend": {
            "traceorder": "normal"
           },
           "missingValuesAsZero": false,
           "numberFormat": "0,0.[00000]",
           "percentFormat": "0[.]00%",
           "series": {
            "error_y": {
             "type": "data",
             "visible": true
            },
            "stacking": null
           },
           "seriesOptions": {
            "column_ef3e62491344": {
             "name": "Iron Feed (%)",
             "type": "line",
             "yAxis": 0
            },
            "column_ef3e62491347": {
             "color": "#00A972",
             "name": "Silica Feed (%)",
             "type": "line",
             "yAxis": 1
            }
           },
           "showDataLabels": false,
           "sizemode": "diameter",
           "sortX": true,
           "sortY": true,
           "swappedAxes": false,
           "textFormat": "",
           "useAggregationsUi": true,
           "valuesOptions": {},
           "version": 2,
           "xAxis": {
            "labels": {
             "enabled": true
            },
            "title": {
             "text": "Date - Time"
            },
            "type": "datetime"
           },
           "yAxis": [
            {
             "title": {
              "text": null
             },
             "type": "-"
            },
            {
             "opposite": true,
             "title": {
              "text": "Silica Feed %"
             },
             "type": "-"
            }
           ]
          }
         }
        ]
       },
       "datasetPreviewNameToCmdIdMap": {},
       "diffDeletes": [],
       "diffInserts": [],
       "displayType": "redashChart",
       "error": null,
       "errorDetails": null,
       "errorSummary": null,
       "errorTraceType": null,
       "finishTime": 0,
       "globalVars": {},
       "guid": "",
       "height": "auto",
       "hideCommandCode": false,
       "hideCommandResult": false,
       "iPythonMetadata": null,
       "inputWidgets": {},
       "isLockedInExamMode": false,
       "latestUser": "a user",
       "latestUserId": null,
       "listResultMetadata": null,
       "metadata": {
        "byteLimit": 2048000,
        "rowLimit": 10000
       },
       "nuid": "9626a56d-0536-463e-a54a-6a914b6792a9",
       "origId": 0,
       "parentHierarchy": [],
       "pivotAggregation": null,
       "pivotColumns": null,
       "position": 1.921875,
       "resultDbfsErrorMessage": null,
       "resultDbfsStatus": "INLINED_IN_TREE",
       "results": null,
       "showCommandTitle": false,
       "startTime": 0,
       "state": "input",
       "streamStates": {},
       "subcommandOptions": {
        "queryPlan": {
         "groups": [
          {
           "column": "column_ef3e62491350",
           "type": "column"
          }
         ],
         "selects": [
          {
           "alias": "column_ef3e62491350",
           "args": [
            {
             "column": "date",
             "type": "column"
            },
            {
             "string": "HOUR",
             "type": "string"
            }
           ],
           "function": "DATE_TRUNC",
           "type": "function"
          },
          {
           "alias": "column_ef3e62491344",
           "args": [
            {
             "column": "Percent_Iron_Feed",
             "type": "column"
            }
           ],
           "function": "AVG",
           "type": "function"
          },
          {
           "alias": "column_ef3e62491347",
           "args": [
            {
             "column": "Percent_Silica_Feed",
             "type": "column"
            }
           ],
           "function": "AVG",
           "type": "function"
          }
         ]
        }
       },
       "submitTime": 0,
       "subtype": "tableResultSubCmd.visualization",
       "tableResultIndex": 0,
       "tableResultSettingsMap": {},
       "useConsistentColors": false,
       "version": "CommandV1",
       "width": "auto",
       "workflows": null,
       "xColumns": null,
       "yColumns": null
      }
     },
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Databricks visualization. Run in Databricks to view."
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1.subcommand+json": {
       "baseErrorDetails": null,
       "bindings": {},
       "collapsed": false,
       "command": "%python\n__backend_agg_display_orig = display\n__backend_agg_dfs = []\ndef __backend_agg_display_new(df):\n    __backend_agg_df_modules = [\"pandas.core.frame\", \"databricks.koalas.frame\", \"pyspark.sql.dataframe\", \"pyspark.pandas.frame\", \"pyspark.sql.connect.dataframe\"]\n    if (type(df).__module__ in __backend_agg_df_modules and type(df).__name__ == 'DataFrame') or isinstance(df, list):\n        __backend_agg_dfs.append(df)\n\ndisplay = __backend_agg_display_new\n\ndef __backend_agg_user_code_fn():\n    import base64\n    exec(base64.standard_b64decode(\"Z29sZF9kZiA9IHNwYXJrLnNxbChmIlNFTEVDVCAqIEZST00ge0RFRkFVTFRfQ0FUQUxPR30ue0RFRkFVTFRfU0NIRU1BfS5nb2xkX2lyb25fb3JlX3ByZWRpY3Rpb25fZGF0YXNldCIpCmRpc3BsYXkoZ29sZF9kZik=\").decode())\n\ntry:\n    # run user code\n    __backend_agg_user_code_fn()\n\n    #reset display function\n    display = __backend_agg_display_orig\n\n    if len(__backend_agg_dfs) > 0:\n        # create a temp view\n        if type(__backend_agg_dfs[0]).__module__ == \"databricks.koalas.frame\":\n            # koalas dataframe\n            __backend_agg_dfs[0].to_spark().createOrReplaceTempView(\"DatabricksView54cf54c\")\n        elif type(__backend_agg_dfs[0]).__module__ == \"pandas.core.frame\" or isinstance(__backend_agg_dfs[0], list):\n            # pandas dataframe\n            spark.createDataFrame(__backend_agg_dfs[0]).createOrReplaceTempView(\"DatabricksView54cf54c\")\n        else:\n            __backend_agg_dfs[0].createOrReplaceTempView(\"DatabricksView54cf54c\")\n        #run backend agg\n        display(spark.sql(\"\"\"WITH q AS (select * from DatabricksView54cf54c) SELECT `date`,AVG(`Percent_Iron_Concentrate`) `column_ef3e62491360`,AVG(`Percent_Silica_Concentrate`) `column_ef3e62491363` FROM q GROUP BY `date`\"\"\"))\n    else:\n        displayHTML(\"dataframe no longer exists. If you're using dataframe.display(), use display(dataframe) instead.\")\n\n\nfinally:\n    spark.sql(\"drop view if exists DatabricksView54cf54c\")\n    display = __backend_agg_display_orig\n    del __backend_agg_display_new\n    del __backend_agg_display_orig\n    del __backend_agg_dfs\n    del __backend_agg_user_code_fn\n\n",
       "commandTitle": "Concentrate",
       "commandType": "auto",
       "commandVersion": 0,
       "commentThread": [],
       "commentsVisible": false,
       "contentSha256Hex": null,
       "customPlotOptions": {
        "redashChart": [
         {
          "key": "type",
          "value": "CHART"
         },
         {
          "key": "options",
          "value": {
           "alignYAxesAtZero": true,
           "coefficient": 1,
           "columnConfigurationMap": {
            "x": {
             "column": "date",
             "id": "column_ef3e62491357"
            },
            "y": [
             {
              "column": "Percent_Iron_Concentrate",
              "id": "column_ef3e62491360",
              "transform": "AVG"
             },
             {
              "column": "Percent_Silica_Concentrate",
              "id": "column_ef3e62491363",
              "transform": "AVG"
             }
            ]
           },
           "dateTimeFormat": "DD/MM/YYYY HH:mm",
           "direction": {
            "type": "counterclockwise"
           },
           "error_y": {
            "type": "data",
            "visible": true
           },
           "globalSeriesType": "line",
           "isAggregationOn": true,
           "legend": {
            "traceorder": "normal"
           },
           "missingValuesAsZero": false,
           "numberFormat": "0,0.[00000]",
           "percentFormat": "0[.]00%",
           "series": {
            "error_y": {
             "type": "data",
             "visible": true
            },
            "stacking": null
           },
           "seriesOptions": {
            "column_ef3e62491360": {
             "name": "Iron Concentrate (%)",
             "type": "line",
             "yAxis": 0
            },
            "column_ef3e62491363": {
             "color": "#00A972",
             "name": "Silica Concentrate (%)",
             "type": "line",
             "yAxis": 1
            }
           },
           "showDataLabels": false,
           "sizemode": "diameter",
           "sortX": true,
           "sortY": true,
           "swappedAxes": false,
           "textFormat": "",
           "useAggregationsUi": true,
           "valuesOptions": {},
           "version": 2,
           "xAxis": {
            "labels": {
             "enabled": true
            },
            "title": {
             "text": "Date - Time"
            },
            "type": "-"
           },
           "yAxis": [
            {
             "title": {
              "text": null
             },
             "type": "-"
            },
            {
             "opposite": true,
             "title": {
              "text": "Silica Concentrate (%)"
             },
             "type": "-"
            }
           ]
          }
         }
        ]
       },
       "datasetPreviewNameToCmdIdMap": {},
       "diffDeletes": [],
       "diffInserts": [],
       "displayType": "redashChart",
       "error": null,
       "errorDetails": null,
       "errorSummary": null,
       "errorTraceType": null,
       "finishTime": 0,
       "globalVars": {},
       "guid": "",
       "height": "auto",
       "hideCommandCode": false,
       "hideCommandResult": false,
       "iPythonMetadata": null,
       "inputWidgets": {},
       "isLockedInExamMode": false,
       "latestUser": "a user",
       "latestUserId": null,
       "listResultMetadata": null,
       "metadata": {
        "byteLimit": 2048000,
        "rowLimit": 10000
       },
       "nuid": "d5d651ab-8d93-436a-9fdf-737249ddbd6c",
       "origId": 0,
       "parentHierarchy": [],
       "pivotAggregation": null,
       "pivotColumns": null,
       "position": 2.921875,
       "resultDbfsErrorMessage": null,
       "resultDbfsStatus": "INLINED_IN_TREE",
       "results": null,
       "showCommandTitle": false,
       "startTime": 0,
       "state": "input",
       "streamStates": {},
       "subcommandOptions": {
        "queryPlan": {
         "groups": [
          {
           "column": "date",
           "type": "column"
          }
         ],
         "selects": [
          {
           "column": "date",
           "type": "column"
          },
          {
           "alias": "column_ef3e62491360",
           "args": [
            {
             "column": "Percent_Iron_Concentrate",
             "type": "column"
            }
           ],
           "function": "AVG",
           "type": "function"
          },
          {
           "alias": "column_ef3e62491363",
           "args": [
            {
             "column": "Percent_Silica_Concentrate",
             "type": "column"
            }
           ],
           "function": "AVG",
           "type": "function"
          }
         ]
        }
       },
       "submitTime": 0,
       "subtype": "tableResultSubCmd.visualization",
       "tableResultIndex": 0,
       "tableResultSettingsMap": {},
       "useConsistentColors": false,
       "version": "CommandV1",
       "width": "auto",
       "workflows": null,
       "xColumns": null,
       "yColumns": null
      }
     },
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Databricks data profile. Run in Databricks to view."
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1.subcommand+json": {
       "baseErrorDetails": null,
       "bindings": {},
       "collapsed": false,
       "command": "%python\nif hasattr(dbutils, \"data\") and hasattr(dbutils.data, \"summarize\"):\n  # setup\n  __data_summary_display_orig = display\n  __data_summary_dfs = []\n  def __data_summary_display_new(df):\n    # add only when result is going to be table type\n    __data_summary_df_modules = [\"pandas.core.frame\", \"databricks.koalas.frame\", \"pyspark.sql.dataframe\", \"pyspark.pandas.frame\", \"pyspark.sql.connect.dataframe\", \"pyspark.sql.classic.dataframe\"]\n    if (type(df).__module__ in __data_summary_df_modules and type(df).__name__ == 'DataFrame') or isinstance(df, list):\n      __data_summary_dfs.append(df)\n  display = __data_summary_display_new\n\n  def __data_summary_user_code_fn():\n    import base64\n    exec(base64.standard_b64decode(\"Z29sZF9kZiA9IHNwYXJrLnNxbChmIlNFTEVDVCAqIEZST00ge0RFRkFVTFRfQ0FUQUxPR30ue0RFRkFVTFRfU0NIRU1BfS5nb2xkX2lyb25fb3JlX3ByZWRpY3Rpb25fZGF0YXNldCIpCmRpc3BsYXkoZ29sZF9kZik=\").decode())\n\n  try:\n    # run user code\n    __data_summary_user_code_fn()\n\n    # run on valid tableResultIndex\n    if len(__data_summary_dfs) > 0:\n      # run summarize\n      if type(__data_summary_dfs[0]).__module__ == \"databricks.koalas.frame\":\n        # koalas dataframe\n        dbutils.data.summarize(__data_summary_dfs[0].to_spark())\n      elif type(__data_summary_dfs[0]).__module__ == \"pandas.core.frame\":\n        # pandas dataframe\n        dbutils.data.summarize(spark.createDataFrame(__data_summary_dfs[0]))\n      else:\n        dbutils.data.summarize(__data_summary_dfs[0])\n    else:\n        displayHTML(\"dataframe no longer exists. If you're using dataframe.display(), use display(dataframe) instead.\")\n\n  finally:\n    display = __data_summary_display_orig\n    del __data_summary_display_new\n    del __data_summary_display_orig\n    del __data_summary_dfs\n    del __data_summary_user_code_fn\nelse:\n  print(\"This DBR version does not support data profiles.\")",
       "commandTitle": "Data Profile",
       "commandType": "auto",
       "commandVersion": 0,
       "commentThread": [],
       "commentsVisible": false,
       "contentSha256Hex": null,
       "customPlotOptions": {},
       "datasetPreviewNameToCmdIdMap": {},
       "diffDeletes": [],
       "diffInserts": [],
       "displayType": "table",
       "error": null,
       "errorDetails": null,
       "errorSummary": null,
       "errorTraceType": null,
       "finishTime": 0,
       "globalVars": {},
       "guid": "",
       "height": "auto",
       "hideCommandCode": false,
       "hideCommandResult": false,
       "iPythonMetadata": null,
       "inputWidgets": {},
       "isLockedInExamMode": false,
       "latestUser": "a user",
       "latestUserId": null,
       "listResultMetadata": [
        [
         "mimeBundle",
         null
        ]
       ],
       "metadata": {
        "byteLimit": 2048000,
        "rowLimit": 10000
       },
       "nuid": "6c060ebd-1658-4808-ab55-326b4f3051cf",
       "origId": 0,
       "parentHierarchy": [],
       "pivotAggregation": null,
       "pivotColumns": null,
       "position": 3.921875,
       "resultDbfsErrorMessage": null,
       "resultDbfsStatus": "INLINED_IN_TREE",
       "results": null,
       "showCommandTitle": false,
       "startTime": 1752017932269,
       "state": "input",
       "streamStates": {},
       "subcommandOptions": null,
       "submitTime": 1752017932269,
       "subtype": "tableResultSubCmd.dataSummary",
       "tableResultIndex": 0,
       "tableResultSettingsMap": {},
       "useConsistentColors": false,
       "version": "CommandV1",
       "width": "auto",
       "workflows": null,
       "xColumns": null,
       "yColumns": null
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "gold_df = spark.sql(f\"SELECT * FROM {DEFAULT_CATALOG}.{DEFAULT_SCHEMA}.gold_iron_ore_prediction_dataset\")\n",
    "display(gold_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "df12fbf7-8766-47a9-aef7-b002812fc823",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Perform Data Cleansing\n",
    "\n",
    "We have identified from the charts above, times where the ore feed % values are constant - either due to data quality issues or missed measurements. We remove these rows from the dataset to ensure that the predictions are only using data whilst the plant was live and both feed and concentrate lab data is available.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aaf8d853-991a-40b4-9d30-9bb75c77550d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Filter out rows between the specified dates\n",
    "start_date = \"2017-05-12\"\n",
    "end_date = \"2017-06-14\"\n",
    "df_filtered = gold_df.filter(~(col(\"date\").between(start_date, end_date)))\n",
    "\n",
    "start_date = \"2017-07-23\"\n",
    "end_date = \"2017-08-03\"\n",
    "df_filtered = df_filtered.filter(~(col(\"date\").between(start_date, end_date)))\n",
    "\n",
    "start_date = \"2017-08-07\"\n",
    "end_date = \"2017-08-14\"\n",
    "df_filtered = df_filtered.filter(~(col(\"date\").between(start_date, end_date)))\n",
    "\n",
    "df_filtered = df_filtered.filter(col(\"Percent_Iron_Feed\") > 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "887f7489-fe94-4449-a3cc-3b536c1cf148",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## üß† 2.2 Feature Store\n",
    "\n",
    "The cleaned feature table from Section 2.1 will now be logged to the Databricks Feature Store, where it can be easily reused for training machine learning models, ensuring consistency and traceability across workflows.\n",
    "\n",
    "Under the hood, the Databricks Feature Store is powered by Delta Tables, offering both performance and flexibility. You can even define feature tables using SQL syntax ‚Äî though for this demo, we‚Äôll use the Databricks Feature Store Python client for simplicity.\n",
    "\n",
    "As highlighted in Section 1, integrating Delta Tables with Unity Catalog brings powerful enterprise-grade capabilities to your feature tables, including:\n",
    "\n",
    "- üîê Row-level filtering and column masking for secure data access\n",
    "- üï∞Ô∏è Time travel and table versioning for reproducibility\n",
    "- üìú Table history and audit logs for compliance and traceability\n",
    "- üîó End-to-end lineage at both table and column level\n",
    "\n",
    "In addition, features can be automatically published to an **online store**, enabling real-time model serving with low-latency access to the latest feature values.\n",
    "\n",
    "You can explore your features in the Feature Registry in the side menu : [Link](https://e2-demo-field-eng.cloud.databricks.com/feature-store?o=1444828305810485)\n",
    "\n",
    "\n",
    "<img src=\"https://www.databricks.com/sites/default/files/2021/12/feature-store-img-2.png?v=1660758008\" style=\"float: right\" width=\"800px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "21b936d7-ff99-43c4-a497-815fe0be5f9e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Let's Create a New Feature Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "7c8b604d-1476-4f66-a750-45b59d3616a7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "feature_table_name = \"fs_gold_iop_features\"\n",
    "\n",
    "# Drop the fs table if it was already existing to cleanup the demo state\n",
    "drop_fs_table(f\"{DEFAULT_CATALOG}.{DEFAULT_SCHEMA}.{feature_table_name}\")\n",
    "\n",
    "# Create feature table using the feature store client\n",
    "fs.create_table(\n",
    "    name=f\"{DEFAULT_CATALOG}.{DEFAULT_SCHEMA}.{feature_table_name}\",\n",
    "    primary_keys=[\"date\"],\n",
    "    df=df_filtered,\n",
    "    description=\"Features for Iron Ore Processing Prediction.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1c8314fb-2299-45f1-8bef-47276261eb18",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Write Feature Table to Catalog\n",
    "\n",
    "Now that the feature table has been generated, we write this to the catalog as a delta table with a primary key of the date column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "37e96566-53c2-430e-86e1-e790228c5777",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#set CDF on feature store table\n",
    "spark.sql(f\"ALTER TABLE {DEFAULT_CATALOG}.{DEFAULT_SCHEMA}.{feature_table_name} SET TBLPROPERTIES (delta.enableChangeDataFeed = true)\")\n",
    "\n",
    "# Create an online table\n",
    "spec = OnlineTableSpec(\n",
    "  primary_key_columns=[\"date\"],\n",
    "  source_table_full_name=f\"{DEFAULT_CATALOG}.{DEFAULT_SCHEMA}.{feature_table_name}\",\n",
    "  run_triggered=OnlineTableSpecTriggeredSchedulingPolicy.from_dict({'triggered': 'true'})\n",
    ")\n",
    "\n",
    "online_table_name = f\"{DEFAULT_CATALOG}.{DEFAULT_SCHEMA}.{feature_table_name}_online_table\"\n",
    "\n",
    "# Check if the online table already exists\n",
    "try:\n",
    "    w.online_tables.get(name=online_table_name)\n",
    "    print(f\"Table {online_table_name} already exists.\")\n",
    "except:\n",
    "    online_table = OnlineTable(\n",
    "      name=online_table_name,  # Fully qualified table name\n",
    "      spec=spec  # Online table specification\n",
    "    )\n",
    "    w.online_tables.create_and_wait(table=online_table)\n",
    "    print(f\"Table {online_table_name} created successfully.\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "02. EDA and Feature Store",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

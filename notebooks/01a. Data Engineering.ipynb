{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "101b5787-3606-457b-b0d1-8a66f5e48943",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# 1. Data Engineering\n",
    "\n",
    "## Data Overview for the Demo\n",
    "\n",
    "The dataset used in this demo comes from Kaggle: [Iron Ore Flotation Data.](https://www.kaggle.com/datasets/edumagalhaes/quality-prediction-in-a-mining-process)\n",
    "\n",
    "It contains two types of data collected from an iron ore flotation plant:\n",
    "\n",
    "- Flotation Process Data\n",
    "  - Includes key operational variables that directly impact the final ore quality.\n",
    "  - This high-frequency data captures the real-time behavior of the flotation process.\n",
    "\n",
    "- Lab Analysis Data \n",
    "  - Includes quality measurements of the feed iron ore before entering the flotation process.\n",
    "  - And includes final quality of the concentrate, measured in the lab.\n",
    "  - Sampled every hour.\n",
    "\n",
    "## Scope of this demo\n",
    "\n",
    "This section of the demo showcases how to implement data engineering best practices on Databricks, split across two notebooks:\n",
    "\n",
    "- Notebook 01a covers:\n",
    "  - Delta Lake: Manage reliable, scalable, and ACID-compliant data pipelines.\n",
    "  - Data Lineage: Understand and trace how data flows from raw inputs to features and predictions.\n",
    "  - Data Quality: Enforce data expectations and monitor quality using built-in tools and frameworks (e.g., Delta Live Tables, expectations).\n",
    "\n",
    "- Notebook 01b covers:\n",
    "  - Unity Catalog: Organise and govern datasets with fine-grained access control and discoverability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bee658c9-8a8b-44c8-86ac-a51ddc84738a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 1.1. Data Pipeline \n",
    "\n",
    "In this example, weâ€™ll implement an end-to-end Delta Live Tables (DLT) pipeline to process Flotation and Lab data. We'll follow the medallion architecture to structure the pipeline, though the same approach can be adapted to support other data modeling patterns such as star schema or data vault.\n",
    "\n",
    "Using Auto Loader, weâ€™ll incrementally ingest new data, enrich it through a series of transformations, and unify it into a curated dataset â€” ready to be leveraged for machine learning and advanced analytics.\n",
    "\n",
    "![Data Lineage](/Workspace/Shared/iron_ore_precessing_demo/demo_setup/images/dlt_graph.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "609c2fd3-c29d-4e41-9256-d513bb790451",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### ðŸ¥‰ Ingest Raw Files into the Bronze Layer\n",
    "\n",
    "We'll use Auto Loader to efficiently ingest raw Parquet files from cloud storage into the bronze layer of our pipeline. Auto Loader is designed to handle millions of files at scale, with built-in support for schema inference and schema evolution, making it ideal for dynamic data environments.\n",
    "\n",
    "ðŸ“š For a deeper dive into Auto Loader, run: dbdemos.install('auto-loader')\n",
    "\n",
    "Letâ€™s now integrate Auto Loader into our pipeline and begin ingesting the raw data arriving in cloud storage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9ffd4133-08f4-4da0-8c90-ef3f30a0ebda",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "\n",
    "catalog_name = spark.conf.get(\"catalog_name\")\n",
    "schema_name = spark.conf.get(\"schema_name\")\n",
    "volume_name = spark.conf.get(\"volume_name\")\n",
    "\n",
    "@dlt.table(\n",
    "    comment=\"Bronze table for flotation data ingested from cloud storage\"\n",
    ")\n",
    "def bronze_flotation():\n",
    "    return (\n",
    "        spark.readStream.format(\"cloudFiles\")\n",
    "        .option(\"cloudFiles.format\", \"parquet\")\n",
    "        .option(\"cloudFiles.inferColumnTypes\", \"true\")\n",
    "        .load(f\"/Volumes/{catalog_name}/{schema_name}/{volume_name}/flotation_data/\")\n",
    "    )\n",
    "\n",
    "@dlt.table(\n",
    "    comment=\"Bronze table for lab data ingested from cloud storage\"\n",
    ")\n",
    "def bronze_lab_data():\n",
    "    return (\n",
    "        spark.readStream.format(\"cloudFiles\")\n",
    "        .option(\"cloudFiles.format\", \"parquet\")\n",
    "        .option(\"cloudFiles.inferColumnTypes\", \"true\")\n",
    "        .load(f\"/Volumes/{catalog_name}/{schema_name}/{volume_name}/lab_data_hourly/\")\n",
    "    )\n",
    "\n",
    "@dlt.table(\n",
    "    comment=\"Bronze table for lab equipment ingested from cloud storage\"\n",
    ")\n",
    "def bronze_lab_equipment():\n",
    "    return (\n",
    "        spark.readStream.format(\"cloudFiles\")\n",
    "        .option(\"cloudFiles.format\", \"parquet\")\n",
    "        .option(\"cloudFiles.inferColumnTypes\", \"true\")\n",
    "        .load(f\"/Volumes/{catalog_name}/{schema_name}/{volume_name}/equipment/\")\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "29fa9acb-d887-4f30-b449-6975e0afd1ce",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### ðŸ¥ˆ Transform and Clean Data into the Silver Layer\n",
    "The Silver layer consumes raw data from the Bronze layer, applying transformations to clean, enrich, and contextualise the information. In this step, weâ€™ll aggregate the Plant Data to an hourly grain to support downstream analytics and modeling.\n",
    "\n",
    "We're also introducing a data quality [expectation](https://docs.databricks.com/workflows/delta-live-tables/delta-live-tables-expectations.html) on the Lab Equipment table to enforce and monitor data quality. This helps ensure that our machine learning models are trained on reliable data, while making it easier to detect and troubleshoot anomalies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f8e0bdb8-50ce-44fd-9843-cf7e43ee31e3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "@dlt.table(\n",
    "    comment=\"Silver view for aggregated flotation data\"\n",
    ")\n",
    "def silver_flotation_data(\n",
    "):\n",
    "    return (\n",
    "        dlt.read(\"bronze_flotation\")\n",
    "        .groupBy(\"date\")\n",
    "        .agg(*[avg(col).alias(col) for col in dlt.read(\"bronze_flotation\").columns if col != \"date\"])\n",
    "    )\n",
    "\n",
    "@dlt.table(\n",
    "    comment=\"Silver view for infeed lab data\"\n",
    ")\n",
    "def silver_infeed_lab_data():\n",
    "    return dlt.read(\"bronze_lab_data\").select(\"date\", \"Percent_Iron_Feed\", \"Percent_Silica_Feed\")\n",
    "\n",
    "\n",
    "@dlt.table(\n",
    "    comment=\"Silver view for concentrate lab data\"\n",
    ")\n",
    "def silver_concentrate_lab_data():\n",
    "    return dlt.read(\"bronze_lab_data\").select(\"date\", \"Percent_Iron_Concentrate\", \"Percent_Silica_Concentrate\", \"operator_name\")\n",
    "    \n",
    "\n",
    "@dlt.table(\n",
    "    comment=\"Silver table for lab equipment\"\n",
    ")\n",
    "@dlt.expect_or_drop(\"is_active\", \"is_active IS NOT NULL\")\n",
    "def silver_lab_equipment():\n",
    "    return (\n",
    "        dlt.read(\"bronze_lab_equipment\").select(\"*\")\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "13db891d-100d-47ba-a084-f9f01422734e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### ðŸ¥‡ Join Clean Datasets into the Gold Layer\n",
    "\n",
    "With our cleaned Silver layer ready, we can now build the Gold layer, where we generate the features needed for our processing prediction model.\n",
    "In this step, weâ€™ll join the Flotation dataset with Lab data to enrich it with key attributes that support predictive modeling of beneficiation performance, including:\n",
    "\n",
    "- ðŸ§ª Infeed Fe and Si content\n",
    "- ðŸŽ¯ Concentrate Fe and Si content\n",
    "\n",
    "This master dataset will serve as the foundation for training and evaluating our machine learning models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "591fdf9f-8318-44d7-b872-30206500ae61",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "@dlt.table(\n",
    "    comment=\"Gold table for iron ore processing features\"\n",
    ")\n",
    "def gold_iron_ore_prediction_dataset():\n",
    "    silver_infeed = dlt.read(\"silver_infeed_lab_data\")\n",
    "    silver_concentrate = dlt.read(\"silver_concentrate_lab_data\")\n",
    "    silver_flotation = dlt.read(\"silver_flotation_data\")\n",
    "    return (\n",
    "        silver_infeed\n",
    "        .join(silver_concentrate, \"date\")\n",
    "        .join(silver_flotation, \"date\")\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8eb50dd6-5b64-4de0-b7b0-cc063f45ce1f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 1.2 Additional ETL and Data Ingestion Capabilities\n",
    "\n",
    "### 1.2.1 LakeFlow connect\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "  <img src=\"../demo_setup/images/LakeFlow1.png\" width=\"800px\"/> \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0fd72708-90ae-4f6d-b189-b09178cf5fdc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "<div style=\"text-align: center;\">\n",
    "  <img src=\"../demo_setup/images/LakeFlow2.png\" width=\"800px\"/> \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4f5d4e26-e8ad-4c44-9e6e-d7fde13b9f15",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "<div style=\"text-align: center;\">\n",
    "  <img src=\"../demo_setup/images/LakeFlow3.png\" width=\"800px\"/> \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e59bf0e5-c7b5-4214-900f-f3464b7ddd9b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 1.2.2 AVEVA Connect\n",
    "\n",
    "Aveva connect allows Delta Sharing with Databricks for easier PI data ingestion. Additional information can be found here:  https://www.databricks.com/dataaisummit/session/unlocking-industrial-intelligence-aveva-and-agnico-eagle\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "  <img src=\"../demo_setup/images/Aveva_connect.png\" width=\"800px\"/> \n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "74ba27c2-eda1-447a-9f9a-a639f53af97b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "<div style=\"text-align: center;\">\n",
    "  <img src=\"../demo_setup/images/Aveva_connect_2.png\" width=\"800px\"/> \n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "01a. Data Engineering",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
